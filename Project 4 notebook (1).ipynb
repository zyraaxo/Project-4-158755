{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491a7d80-bc85-4977-9b4c-de428cd1fcea",
   "metadata": {},
   "source": [
    "# Data journalism: data visualisation â€“ implementation of interactive graphs (web enabled), infographics.\n",
    "\n",
    "This notebook explores how sentiment and metadata from social media posts can be used to predict user engagement (likes + retweets). We also correlate trending news topics to online activity. This will help jouranlists find tredning topics via Social media and see how they effect each other. \n",
    "\n",
    "This project delivers a real-time trend forecasting web app that analyzes world-related hashtags (e.g., #Fitness, #Climate Change, #Ukraine) on X and Reddit posts combined. It combines social media data with current news headlines (via the News API) Using NLP and machine learning, it extracts trending keywords, predicts post engagement (likes and retweets, upvotes), and forecasts topic popularity over 24â€“48 hours. The tool is deployed as an interactive Streamlit dashboard, offering visualizations like word clouds and trend curves. A Jupyter notebook documents the full data science workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042c924-9eb6-4a26-bf8f-0bbca05552ee",
   "metadata": {},
   "source": [
    "# Problem Statement: \n",
    "\n",
    "Trends on social media emerge and fade rapidly. Marketers, journalists, and researchers often struggle to anticipate these shifts. This project addresses that challenge by forecasting trend lifecycles, helping users optimize content timing and stay ahead of competitors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85e339-cfa0-4fb3-a88b-0c0f7c0b2963",
   "metadata": {},
   "source": [
    "## Objectives: \n",
    "\n",
    "1. To collect and preprocess real-time social media data from X (formerly Twitter) and Reddit, focusing on globally relevant hashtags (e.g., #ClimateChange, #Ukraine, #Fitness), along with current news headlines using the NewsAPI.\n",
    "2. To perform sentiment analysis and keyword extraction on social media posts and news headlines using Natural Language Processing (NLP) techniques.\n",
    "3. To develop predictive models that estimate user engagement, such as likes, retweets, and upvotes, based on post content, sentiment, and metadata (e.g., time posted, hashtag used).\n",
    "4. To forecast the popularity of trending topics over a 24â€“48 hour period using time series analysis and trend modeling.\n",
    "5. To analyze the correlation between news coverage and online social media activity, highlighting how news drives or reflects online trends.\n",
    "6. To build and deploy an interactive Streamlit dashboard that:\n",
    "\n",
    "#### Displays real-time trends,\n",
    "\n",
    "#### Visualizes sentiment and keyword patterns (e.g., word clouds, trend curves),\n",
    "\n",
    "#### Allows journalists and users to explore topic impact and forecast engagement.\n",
    "\n",
    "7. To Document process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa87ddd-f58d-4364-a8b7-77337651474b",
   "metadata": {},
   "source": [
    "### Libraries Needed: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8636cf92-55a1-43e9-9037-fe2550c12f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import praw\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b1c63-5c70-4a43-8884-fbb987bf3411",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1be5d-64e4-40fb-9512-3e9a158132bd",
   "metadata": {},
   "source": [
    "## Data sources: \n",
    "For social media, we use the X api [1]. This allows you gather posts from X within defined parameters. This will be done using hashtags these usally represent trending topics [2]. The newsAPI [3] will be used to gather news articles based on the paramters from the X posts aswell as Reddit [4] posts, for example #Fitness retreived the posts will be the search paramter for the news posts. \n",
    "\n",
    "## *Please add other data sources here if used*\n",
    "\n",
    "\n",
    "1. https://docs.x.com/x-api/introduction\n",
    "2. https://www.shopify.com/nz/blog/twitter-hashtags\n",
    "3. https://newsapi.org/\n",
    "4. https://www.reddit.com/dev/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771291d-831c-425b-a4e9-55f20fc3a076",
   "metadata": {},
   "source": [
    "# Limitations: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7acae-8de3-4650-8137-d2b962091831",
   "metadata": {},
   "source": [
    "Within the bounds of the X api free acount you are entitled to 100 posts from X, with Reddit its a lot more but capped to 60 requests a minute. The combined dataset will give us roughly 600 bits of data to work with with the abilty to add more from reddit when needed. This may skew the data towards Reddit posts but by doing the sentiment scores it will average out over all the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f72a1-4f6a-42a6-85e8-3578298e7851",
   "metadata": {},
   "source": [
    "# Ethical data usage: \n",
    "\n",
    "\n",
    "### X: \n",
    "The X API can be used for a university project if it aligns with Xâ€™s License Agreement, prioritizing user privacy, transparency, and ethical data use while avoiding harmful applications like misinformation or unauthorized data scraping. Ensure compliance with platform policies and secure data handling, especially for public interest research, though access may require navigating paid tiers or specific approvals under regulations like the EUâ€™s DSA. (https://developer.x.com/en/developer-terms/agreement-and-policy) \n",
    "\n",
    "### NewsAPI: \n",
    "The News API (https://newsapi.org/terms) can be ethically used for a university project by adhering to its terms, which require lawful data use, compliance with local regulations, and respecting intellectual property through proper source attribution. Ensure transparency, secure handling of the API key, and limit data use to non-commercial academic purposes within the free tierâ€™s 500 requests/day, avoiding unauthorized redistribution of licensed content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5ee8f-e972-4d8b-b0d8-468b31c38b22",
   "metadata": {},
   "source": [
    "### Here is how the NewsAPI is used. This wont run on this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bda8c46-05e4-41aa-94e5-e7e9f2f42c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NewsApiClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Initialize News API ---\u001b[39;00m\n\u001b[32m      2\u001b[39m API_KEY = \u001b[33m\"\u001b[39m\u001b[33m7af7d5e56edc4148aac908f2c9f86ac3\u001b[39m\u001b[33m\"\u001b[39m  \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m newsapi = \u001b[43mNewsApiClient\u001b[49m(api_key=API_KEY)\n\u001b[32m      5\u001b[39m st.title(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Real-Time Social + News Dashboard with Engagement Forecasting\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- User Topic Input ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'NewsApiClient' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Initialize News API ---\n",
    "API_KEY = \"7af7d5e56edc4148aac908f2c9f86ac3\"  \n",
    "newsapi = NewsApiClient(api_key=API_KEY)\n",
    "\n",
    "st.title(\"ðŸ“Š Real-Time Social + News Dashboard with Engagement Forecasting\")\n",
    "\n",
    "# --- User Topic Input ---\n",
    "topic = st.text_input(\"Enter a topic keyword (e.g., #Fitness, climate change):\", \"#Fitness\")\n",
    "\n",
    "# --- News Fetching ---\n",
    "if topic:\n",
    "    with st.spinner(\"Fetching news articles...\"):\n",
    "        all_articles = newsapi.get_everything(\n",
    "            q=topic,\n",
    "            language='en',\n",
    "            sort_by='publishedAt',\n",
    "            page_size=10\n",
    "        )\n",
    "    articles = all_articles.get('articles', [])\n",
    "\n",
    "    st.header(f\"ðŸ“° Latest News on {topic}\")\n",
    "    if articles:\n",
    "        for article in articles:\n",
    "            st.subheader(article['title'])\n",
    "            st.write(article['description'])\n",
    "            st.markdown(f\"[Read more]({article['url']})\")\n",
    "            st.write(f\"Published at: {article['publishedAt']}\")\n",
    "            st.markdown(\"---\")\n",
    "    else:\n",
    "        st.write(\"No news articles found for this topic.\")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "@st.cache_data\n",
    "def load_social_data():\n",
    "    df = pd.read_csv(\"data/x_posts_with_weather.csv\")\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "df = load_social_data()\n",
    "\n",
    "# --- Filter Dataset ---\n",
    "if topic:\n",
    "    mask = df['hashtags'].str.contains(topic.replace(\"#\", \"\"), case=False, na=False)\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    st.header(f\"ðŸ“± Social Media Posts on {topic}\")\n",
    "    st.write(f\"Total posts found: {filtered_df.shape[0]}\")\n",
    "\n",
    "    if not filtered_df.empty:\n",
    "        st.line_chart(filtered_df.groupby(filtered_df['created_at'].dt.floor('H')).size())\n",
    "    else:\n",
    "        st.write(\"No social media posts found for this topic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975bdf8d-f67a-4639-9d05-a7b46a3c97ab",
   "metadata": {},
   "source": [
    "# Reddit Data: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f8747-f06e-4342-95b7-9f5238428d91",
   "metadata": {},
   "source": [
    "Reddit uses a libary named 'praw', this is an API wrapper for reddit and is what receives the posts from Reddit. It does require an client ID, client_secret and user_agent which is given to you when you create an app through Reddit developer. \n",
    "\n",
    "\n",
    "https://praw.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86371cdb-3327-437b-91a2-a28436270f02",
   "metadata": {},
   "source": [
    "## Code Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b76652-a609-47f5-950b-8ef176436399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='v5b2CYNg37amXniM43bNmQ',\n",
    "    client_secret='cqVeL5VR-vENbiLAjnfC-xoRn45qaQ',\n",
    "    user_agent=\"MyRedditSentimentApp/0.1 by noahcrampton\"\n",
    ")\n",
    "\n",
    "subreddit = reddit.subreddit(\"all\")\n",
    "posts = []\n",
    "\n",
    "# You can use .hot(), .new(), or .top(), ill use hot() to get treding postss\n",
    "for post in subreddit.new(limit=500):\n",
    "    posts.append({\n",
    "        \"title\": post.title,\n",
    "        \"score\": post.score,\n",
    "        \"comments\": post.num_comments,\n",
    "        \"created\": datetime.utcfromtimestamp(post.created_utc),\n",
    "        \"url\": post.url,\n",
    "        \"selftext\": post.selftext,\n",
    "        \"subreddit\": str(post.subreddit)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(posts).drop_duplicates(subset=\"title\")\n",
    "df.sort_values(by=\"created\", ascending=False, inplace=True)\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "df.to_csv(\"data/reddit_all_recent_posts.csv\", index=False)\n",
    "print(\"Saved to reddit_all_recent_posts.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d265f-bc8d-4973-854e-98f44a1c76ee",
   "metadata": {},
   "source": [
    "# Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ac6e3-e03f-4115-9dd8-6d15c60419cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63bfca54-2653-46bf-aad3-ad0c8ce1e92b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e921a18-415e-4b32-8ee6-7b0261cb7fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3953a37a-9a4c-49a4-8bb5-11bc6b3a5af6",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f883468-b0f1-4cd9-ad7a-1442c8adc100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bc41de1-0b2c-46f2-8839-b001ebe11476",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ef228-facc-49cf-a5c2-8a26b191705c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60bb6ec9-2040-44a4-ab3f-80d8a0d3aa7a",
   "metadata": {},
   "source": [
    "# Forecasting & Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4752221-b5d7-4375-b168-82c76a0c605e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c39f4544-9030-4fff-94b2-99763496387d",
   "metadata": {},
   "source": [
    "# Streamlit App Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733d9eb-ad5b-4bac-8742-9b0ac539a48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "485d3e44-22f8-4685-9e9a-3fcf4b028300",
   "metadata": {},
   "source": [
    "# Insights & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41d698-f39b-4371-bad4-353b18b96d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
